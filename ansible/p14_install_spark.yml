---
# Playbook 17: Install and Configure Spark
# This playbook installs and configures Apache Spark on management and worker nodes. 
# It downloads Spark binaries, configures environment variables, and ensures proper setup for the Spark cluster.

- name: Notify user about Playbook 18
  hosts: localhost
  tasks:
    - name: Display playbook message
      ansible.builtin.debug:
        msg: "Running Playbook 17: Install and Configure Spark - This playbook installs Apache Spark, configures environment variables, and ensures Spark is properly set up on management and worker nodes."

- name: Install and configure Spark
  hosts: management, workers
  become: true  # Enable privilege escalation for all tasks
  become_user: almalinux  # Run as 'almalinux' user
  tasks:
    # Step 1: Download Spark binaries
    - name: Download Spark binaries locally
      # Download the Spark binary archive to the control node.
      delegate_to: localhost
      run_once: true
      ansible.builtin.get_url:
        dest: /tmp/spark.tgz
        url: "https://archive.apache.org/dist/spark/spark-3.5.3/spark-3.5.3-bin-hadoop3-scala2.13.tgz"
        timeout: 180
      retries: 3
      delay: 30

    # Step 2: Verify Spark archive exists on control node
    - name: Verify Spark archive exists on control node
      # Check if the Spark archive was successfully downloaded to the control node.
      delegate_to: localhost
      run_once: true
      ansible.builtin.stat:
        path: /tmp/spark.tgz
      register: spark_archive_local

    - name: Fail if Spark archive does not exist on control node
      # Fail the playbook if the Spark archive does not exist on the control node.
      delegate_to: localhost
      run_once: true
      ansible.builtin.fail:
        msg: "Spark archive was not downloaded successfully on the control node!"
      when: not spark_archive_local.stat.exists

    # Step 3: Copy Spark binaries to target nodes
    - name: Copy Spark binaries to nodes
      # Copy the Spark archive from the control node to the target nodes.
      ansible.builtin.copy:
        src: /tmp/spark.tgz
        dest: /home/almalinux/spark.tgz

    # Step 4: Verify Spark archive exists on target nodes
    - name: Verify Spark archive exists on nodes
      # Check if the Spark archive was successfully copied to the target nodes.
      ansible.builtin.stat:
        path: /home/almalinux/spark.tgz
      register: spark_archive_remote

    - name: Fail if Spark archive does not exist on nodes
      # Fail the playbook if the Spark archive does not exist on the target nodes.
      ansible.builtin.fail:
        msg: "Spark archive was not copied to the nodes!"
      when: not spark_archive_remote.stat.exists

    # Step 5: Extract Spark binaries
    - name: Extract Spark binaries
      # Extract the Spark binary archive on the target nodes.
      ansible.builtin.unarchive:
        src: /home/almalinux/spark.tgz
        dest: /home/almalinux/
        remote_src: true
        owner: almalinux
        group: almalinux

    # Step 6: Ensure Spark directory ownership
    - name: Set ownership of Spark directory (if required)
      # Ensure the extracted Spark directory is owned by the 'almalinux' user and group.
      ansible.builtin.file:
        path: /home/almalinux/spark-3.5.3-bin-hadoop3-scala2.13
        state: directory
        recurse: true
        owner: almalinux
        group: almalinux

    # Step 7: Configure Spark defaults
    - name: Apply Spark configuration
      # Deploy the Spark configuration file (`spark-defaults.conf`) from a template.
      ansible.builtin.template:
        src: "templates/spark-defaults.conf.j2"  # Dynamic path to the template file.
        dest: /home/almalinux/spark-3.5.3-bin-hadoop3-scala2.13/conf/spark-defaults.conf
        owner: almalinux
        group: almalinux

    # Step 8: Ensure spark-env.sh exists
    - name: Ensure spark-env.sh exists
      # Ensure the `spark-env.sh` file exists in the Spark configuration directory.
      ansible.builtin.file:
        path: /home/almalinux/spark-3.5.3-bin-hadoop3-scala2.13/conf/spark-env.sh
        state: touch
        mode: '0644'
        owner: almalinux
        group: almalinux

    # Step 9: Configure Spark environment variables
    - name: Configure Spark environment
      # Add necessary environment variables for Spark in `spark-env.sh`.
      ansible.builtin.blockinfile:
        path: /home/almalinux/spark-3.5.3-bin-hadoop3-scala2.13/conf/spark-env.sh
        block: |
          SPARK_MASTER_HOST={{ groups['management'][0] }}
          SPARK_LOCAL_IP={{ ansible_default_ipv4.address }}
        insertafter: EOF
        owner: almalinux
        group: almalinux